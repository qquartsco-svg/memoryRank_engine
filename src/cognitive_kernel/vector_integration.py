"""
ðŸ”— Vector DB Integration for Cognitive Kernel

Vector DB (Chroma/FAISS)ë¥¼ ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ ê¸°ì–µ(Semantic Memory)ì„ ì €ìž¥í•˜ê³ ,
Cognitive Kernelì˜ MemoryRankë¡œ ì¤‘ìš”ë„ ìž¬ëž­í‚¹í•˜ëŠ” í†µí•© ëª¨ë“ˆ.

êµ¬ì¡°:
    [Embedding Model] â†’ [Vector DB] â†’ [MemoryRank] â†’ [PFC]

Author: GNJz (Qquarts)
Version: 2.0.0
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

try:
    import chromadb
    from chromadb.config import Settings
    CHROMA_AVAILABLE = True
except ImportError:
    CHROMA_AVAILABLE = False

try:
    import faiss
    import numpy as np
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer
    EMBEDDING_AVAILABLE = True
except ImportError:
    EMBEDDING_AVAILABLE = False


class VectorDBBackend:
    """
    Vector DB ë°±ì—”ë“œ ì¶”ìƒ í´ëž˜ìŠ¤
    
    Cognitive Kernelì˜ remember() ë©”ì„œë“œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ embeddingí•˜ì—¬
    Vector DBì— ì €ìž¥í•˜ê³ , recall()ì—ì„œ semantic searchë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, backend_type: str = "chroma", **kwargs):
        """
        Args:
            backend_type: "chroma" or "faiss"
            **kwargs: ë°±ì—”ë“œë³„ ì„¤ì •
        """
        self.backend_type = backend_type
        self.embedding_model = None
        self._init_backend(**kwargs)
    
    def _init_backend(self, **kwargs):
        """ë°±ì—”ë“œ ì´ˆê¸°í™”"""
        if self.backend_type == "chroma":
            if not CHROMA_AVAILABLE:
                raise ImportError("chromadb not installed. pip install chromadb")
            self._init_chroma(**kwargs)
        elif self.backend_type == "faiss":
            if not FAISS_AVAILABLE:
                raise ImportError("faiss-cpu not installed. pip install faiss-cpu")
            self._init_faiss(**kwargs)
        else:
            raise ValueError(f"Unknown backend: {self.backend_type}")
        
        # Embedding ëª¨ë¸ ì´ˆê¸°í™”
        if EMBEDDING_AVAILABLE:
            model_name = kwargs.get("embedding_model", "all-MiniLM-L6-v2")
            self.embedding_model = SentenceTransformer(model_name)
        else:
            raise ImportError("sentence-transformers not installed. pip install sentence-transformers")
    
    def _init_chroma(self, path: str = "./chroma_db", collection_name: str = "cognitive_memory"):
        """Chroma DB ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=path,
            settings=Settings(anonymized_telemetry=False)
        )
        self.collection = self.client.get_or_create_collection(
            name=collection_name,
            metadata={"description": "Cognitive Kernel semantic memory"}
        )
        self.path = path
    
    def _init_faiss(self, dimension: int = 384, path: str = "./faiss_index"):
        """FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”"""
        self.dimension = dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.ids = []  # ID ë¦¬ìŠ¤íŠ¸
        self.metadata = []  # ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        self.path = path
    
    def embed(self, text: str) -> List[float]:
        """í…ìŠ¤íŠ¸ë¥¼ embedding ë²¡í„°ë¡œ ë³€í™˜"""
        if self.embedding_model is None:
            raise RuntimeError("Embedding model not initialized")
        return self.embedding_model.encode(text).tolist()
    
    def add_memory(
        self,
        memory_id: str,
        text: str,
        metadata: Dict[str, Any],
        importance: float = 0.5
    ) -> None:
        """ê¸°ì–µì„ Vector DBì— ì¶”ê°€"""
        embedding = self.embed(text)
        
        if self.backend_type == "chroma":
            self.collection.add(
                ids=[memory_id],
                embeddings=[embedding],
                documents=[text],
                metadatas=[{
                    **metadata,
                    "importance": importance,
                    "text": text
                }]
            )
        elif self.backend_type == "faiss":
            # FAISSëŠ” numpy array í•„ìš”
            embedding_array = np.array([embedding], dtype=np.float32)
            self.index.add(embedding_array)
            self.ids.append(memory_id)
            self.metadata.append({
                **metadata,
                "importance": importance,
                "text": text
            })
    
    def search(
        self,
        query: str,
        k: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Semantic search ìˆ˜í–‰
        
        Returns:
            List of {id, text, metadata, distance, importance}
        """
        query_embedding = self.embed(query)
        
        if self.backend_type == "chroma":
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=k,
                where=filter_metadata
            )
            
            memories = []
            if results["ids"] and len(results["ids"][0]) > 0:
                for i in range(len(results["ids"][0])):
                    memories.append({
                        "id": results["ids"][0][i],
                        "text": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i],
                        "distance": results["distances"][0][i] if "distances" in results else None,
                        "importance": results["metadatas"][0][i].get("importance", 0.5)
                    })
            return memories
        
        elif self.backend_type == "faiss":
            query_array = np.array([query_embedding], dtype=np.float32)
            distances, indices = self.index.search(query_array, k)
            
            memories = []
            for idx, dist in zip(indices[0], distances[0]):
                if idx < len(self.ids):
                    memories.append({
                        "id": self.ids[idx],
                        "text": self.metadata[idx].get("text", ""),
                        "metadata": self.metadata[idx],
                        "distance": float(dist),
                        "importance": self.metadata[idx].get("importance", 0.5)
                    })
            return memories
    
    def save(self, path: Optional[Path] = None):
        """Vector DB ìƒíƒœ ì €ìž¥ (FAISSë§Œ í•„ìš”)"""
        if self.backend_type == "faiss":
            if path is None:
                path = Path(self.path)
            path.mkdir(parents=True, exist_ok=True)
            faiss.write_index(self.index, str(path / "index.faiss"))
            
            # ë©”íƒ€ë°ì´í„° ì €ìž¥
            with open(path / "metadata.json", "w") as f:
                json.dump({
                    "ids": self.ids,
                    "metadata": self.metadata,
                    "dimension": self.dimension
                }, f, ensure_ascii=False, indent=2)
    
    def load(self, path: Optional[Path] = None):
        """Vector DB ìƒíƒœ ë¡œë“œ (FAISSë§Œ í•„ìš”)"""
        if self.backend_type == "faiss":
            if path is None:
                path = Path(self.path)
            
            # ì¸ë±ìŠ¤ ë¡œë“œ
            self.index = faiss.read_index(str(path / "index.faiss"))
            
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            with open(path / "metadata.json", "r") as f:
                data = json.load(f)
                self.ids = data["ids"]
                self.metadata = data["metadata"]
                self.dimension = data["dimension"]

