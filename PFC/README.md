# PFC Engine (Prefrontal Cortex)

> **"ë¬´ì—‡ì„ ê¸°ì–µí•˜ê³ , ì–´ë–»ê²Œ í–‰ë™í• ì§€ ê²°ì •í•˜ëŠ” ê°ë…"** â€” ìž‘ì—… ê¸°ì–µ + í–‰ë™ ì„ íƒ + ì–µì œ ì—”ì§„

---

## ðŸŽ¬ ê¸°ì–µì˜ ì˜í™”ê´€ì—ì„œì˜ ì—­í• 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ§  Cognitive Kernel                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ðŸŽžï¸ Panorama (í•„ë¦„)    â†’   "ë¬´ìŠ¨ ì¼ì´ ìžˆì—ˆë‚˜"               â”‚
â”‚   ðŸ’¡ MemoryRank (ì¡°ê´‘ê¸°) â†’   "ë­ê°€ ì¤‘ìš”í•œê°€"                  â”‚
â”‚   ðŸŽ¬ PFC (ì˜ì‚¬ê¸°+ê°ë…)   â†’   "ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€" â† ì´ ì—”ì§„     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## í•µì‹¬ ê¸°ëŠ¥ (v1.0)

| ê¸°ëŠ¥ | ì„¤ëª… | ìˆ˜ì‹ |
|------|------|------|
| **Working Memory** | ì¤‘ìš” ì •ë³´ ìž„ì‹œ ì €ìž¥ (Miller's Law: 7Â±2) | capacity eviction |
| **Action Evaluator** | í–‰ë™ì˜ ê¸°ëŒ€ íš¨ìš© ê³„ì‚° | U = r - c - riskÃ—Îº |
| **Inhibitor** | ìœ„í—˜í•œ í–‰ë™ ì–µì œ (Go/No-Go) | conflict > threshold |
| **Selector** | Softmax í™•ë¥ ì  ì„ íƒ | P(i) = exp(Î²U_i) / Î£exp(Î²U_j) |

---

## Quick Start

\`\`\`python
from pfc import PFCEngine, PFCConfig, Action

# ì—”ì§„ ì´ˆê¸°í™”
pfc = PFCEngine(PFCConfig(
    working_memory_capacity=7,
    risk_aversion=0.5,
    inhibition_threshold=0.7,
    decision_temperature=1.0,
))

# MemoryRank ê²°ê³¼ ë¡œë“œ
top_memories = [("memory_001", 0.45), ("memory_002", 0.30)]
pfc.load_from_memoryrank(top_memories)

# í–‰ë™ í›„ë³´ ì •ì˜
actions = [
    Action.create("rest", reward=0.6, cost=0.1, risk=0.05),
    Action.create("work", reward=0.8, cost=0.5, risk=0.2),
    Action.create("risky", reward=0.9, cost=0.4, risk=0.8),
]

# í†µí•© ì²˜ë¦¬
result = pfc.process(
    candidate_actions=actions,
    goal="complete daily tasks",
)

if result.inhibited:
    print("í–‰ë™ ì–µì œë¨")
else:
    print(f"ì„ íƒ: {result.action.name}, íš¨ìš©: {result.utility:.3f}")
\`\`\`

---

## Output Example

\`\`\`
============================================================
PFC Engine v1.0 - Test (ì˜ì‚¬ê¸° + ê°ë…)
============================================================

[1] Working Memory í…ŒìŠ¤íŠ¸ (Miller's Law: ìš©ëŸ‰ 5)
  ë¡œë“œëœ ê¸°ì–µ ìˆ˜: 5 (ìš©ëŸ‰: 5)
    - memory_trauma_001: relevance=0.900
    - memory_yesterday_lunch: relevance=0.600

[2] í–‰ë™ í›„ë³´ íš¨ìš© í‰ê°€
  rest: U = 0.475 (r=0.6, c=0.1, risk=0.05)
  work: U = 0.200 (r=0.8, c=0.5, risk=0.2)
  risky_adventure: U = 0.100 (r=0.9, c=0.4, risk=0.8)

[3] ì–µì œ(Inhibition) í…ŒìŠ¤íŠ¸
  'risky_adventure' ì–µì œ ì—¬ë¶€: True
  ê°ˆë“± ì‹ í˜¸: 0.800 (threshold: 0.6)

[4] Softmax í–‰ë™ ì„ íƒ
  rest: 42.4%
  work: 24.5%
  socialize: 33.1%

âœ… PFC Engine í…ŒìŠ¤íŠ¸ ì™„ë£Œ
\`\`\`

---

## API Reference

### PFCConfig

| íŒŒë¼ë¯¸í„° | íƒ€ìž… | ê¸°ë³¸ê°’ | ì„¤ëª… |
|---------|------|-------|------|
| working_memory_capacity | int | 7 | Miller's Law ìš©ëŸ‰ |
| decay_rate | float | 0.1 | ìž‘ì—… ê¸°ì–µ ê°ì‡ ìœ¨ |
| risk_aversion | float | 0.5 | ìœ„í—˜ íšŒí”¼ ê³„ìˆ˜ (Îº) |
| inhibition_threshold | float | 0.7 | ì–µì œ ìž„ê³„ê°’ |
| decision_temperature | float | 1.0 | Softmax ì˜¨ë„ (Î²) |

### PFCEngine

| ë©”ì„œë“œ | ì„¤ëª… |
|--------|------|
| load_to_working_memory() | ìž‘ì—… ê¸°ì–µì— í•­ëª© ì¶”ê°€ |
| load_from_memoryrank() | MemoryRank ê²°ê³¼ ë¡œë“œ |
| evaluate_action() | í–‰ë™ íš¨ìš© ê³„ì‚° |
| should_inhibit() | ì–µì œ ì—¬ë¶€ íŒë‹¨ |
| select_action() | Softmax í–‰ë™ ì„ íƒ |
| process() | í†µí•© íŒŒì´í”„ë¼ì¸ |

---

## Algorithm Details

### Expected Utility

\`\`\`
U(action) = expected_reward - effort_cost - risk Ã— risk_aversion
\`\`\`

### Softmax Selection

\`\`\`
P(action_i) = exp(Î² Ã— U_i) / Î£ exp(Î² Ã— U_j)

Î² = decision_temperature
\`\`\`

### Working Memory Decay

\`\`\`
relevance(t) = relevance_0 Ã— exp(-Î» Ã— Î”t)
\`\`\`

---

## í™œìš© ì‹œë‚˜ë¦¬ì˜¤

| ë¶„ì•¼ | í™œìš© |
|------|------|
| **AI ì—ì´ì „íŠ¸** | ëŒ€í™” ë§¥ë½ ìœ ì§€, í–‰ë™ ê²°ì • |
| **ADHD ì‹œë®¬ë ˆì´ì…˜** | ìž‘ì—… ê¸°ì–µ ìš©ëŸ‰ ê°ì†Œ, ì–µì œ ì•½í™” |
| **ìš°ìš¸ì¦ ì‹œë®¬ë ˆì´ì…˜** | í–‰ë™ íš¨ìš© ì™œê³¡, ë¬´ê¸°ë ¥ |
| **ê²Œìž„ NPC** | ëª©í‘œ ê¸°ë°˜ í–‰ë™ ì„ íƒ |

---

## License

MIT License

---

## PHAM Blockchain Signature

| í•­ëª© | ê°’ |
|------|---|
| Author | GNJz (Qquarts) |
| Date | 2025-01-29 |
| Version | v1.0.0 |

---

---

# English Version

> [ðŸ‡°ðŸ‡· í•œêµ­ì–´](#pfc-engine-prefrontal-cortex) | **ðŸ‡ºðŸ‡¸ English**

> **"What to remember, how to act"** â€” Working memory + Action selection + Inhibition engine

---

## ðŸŽ¬ Role in Memory Theater

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ðŸ§  Cognitive Kernel                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ðŸŽžï¸ Panorama (Film)     â†’   "What happened?"               â”‚
â”‚   ðŸ’¡ MemoryRank (Dimmer)  â†’   "What matters?"                â”‚
â”‚   ðŸŽ¬ PFC (Director)       â†’   "What to do?" â† This Engine   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Features (v1.0)

| Feature | Description | Formula |
|---------|-------------|---------|
| **Working Memory** | Store important info temporarily (Miller's Law: 7Â±2) | Capacity eviction |
| **Action Evaluator** | Calculate expected utility | U = r - c - riskÃ—Îº |
| **Inhibitor** | Suppress risky actions (Go/No-Go) | conflict > threshold |
| **Selector** | Softmax probabilistic selection | P(i) = exp(Î²U_i) / Î£exp(Î²U_j) |

---

## ðŸš€ Quick Start

```python
from pfc import PFCEngine, PFCConfig, Action

pfc = PFCEngine(PFCConfig(
    working_memory_capacity=7,
    risk_aversion=0.5,
    inhibition_threshold=0.7,
))

# Load from MemoryRank
pfc.load_from_memoryrank([("mem_001", 0.45), ("mem_002", 0.30)])

# Define action candidates
actions = [
    Action.create("rest", reward=0.7, cost=0.1, risk=0.05),
    Action.create("work", reward=0.8, cost=0.5, risk=0.2),
]

# Select action
result = pfc.select_action(actions)
print(f"Selected: {result.action.name}, Utility: {result.utility:.3f}")
```

---

## ðŸ”¬ Algorithm Details

### Expected Utility

```
U(action) = expected_reward - effort_cost - risk Ã— risk_aversion
```

### Softmax Selection

```
P(action_i) = exp(Î² Ã— U_i) / Î£ exp(Î² Ã— U_j)
Î² = decision_temperature
```

### Working Memory Decay

```
relevance(t) = relevance_0 Ã— exp(-Î» Ã— Î”t)
```

---

## ðŸŽ¯ Use Cases

| Domain | Application |
|--------|-------------|
| **AI Agents** | Maintain conversation context, make decisions |
| **ADHD Simulation** | Reduced working memory, weakened inhibition |
| **Depression Simulation** | Distorted action utility, lethargy |
| **Game NPC** | Goal-based action selection |

---

## ðŸ“„ License

MIT License

---

## âœ… PHAM Blockchain Signature

Signed with **PHAM (Proof of Honest Authorship & Merit)**.

---

**Author**: GNJz (Qquarts)
